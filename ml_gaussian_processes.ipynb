{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given some input $X$ and we wanna predict some output $y=f(x)$.\n",
    "\n",
    "My dataset will be pairs of $(x_i, y_i)$, and we will use it to predict an unseen point $(x^{*}, y^{*})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "How to construct a covariance matrix that specifies that some points are more similar than others, like in the following case, where $x_1$ is more similar to $x_2$ than to $x_3$:\n",
    "\n",
    "<img src='img/ml_gaussian_processes_cov.png' />\n",
    "\n",
    "The way to do it is to construct a covariance matrix where each entry is determined by a similarity measurement, such as:\n",
    "\n",
    "$$\n",
    "k_{ij} = \\exp (-||x_i - x_j||^2)\n",
    "$$\n",
    "\n",
    "This kernel has the property that, if $x_i$ is very disimilar to $x_j$, then $k_{ij} = 0$; and if they are exactly the same, then $k_{ij} = 1$.\n",
    "\n",
    "The kernel will determine how smooth the function is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on previous datapoints \n",
    "\n",
    "I will assume that my datapoints are all drawn from the same distribution: $y \\sim N(0, K)$.\n",
    "And I will also assume that unseen points will be drawn from another distribution (that is also zero-centered, but has a diferent covariance) $y_* \\sim N(0, K_{*}$).\n",
    "Expressed this way, these random vars are independent, but in reality, we would like these $y$ and these $y_*$ to be correlated, and so we will need to specify this relation in a new covariance matrix, such that:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "y_*\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "N(0,\n",
    "\\begin{bmatrix}\n",
    "k_{11} & ... & k_{1n} & k_{1*} \\\\\n",
    "k_{21} & ... & k_{2n} & k_{2*} \\\\\n",
    "k_{n1} & ... & k_{nn} & k_{n*} \\\\\n",
    "k_{*1} & ... & k_{*n} & k_{**}\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "From the above matrix, some of these things we already have (namely, the dataset covariance matrix $K$); what we will need is to compute the vector $k_{i*}$ and the value $k_{**}$, and then use the theorem for constructing _conditional Gaussians_:\n",
    "\n",
    "<img src='img/ml_gaussian_processes_new_cov.png' />\n",
    "\n",
    "So, using the theorem, for each point in the test set, we can compute the mean and the variance (and thus get some desired confidence interval for the predicted value):\n",
    "\n",
    "<img src='img/ml_gaussian_processes_mean_conf.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes\n",
    "\n",
    "GPs are distributions over functions: $f(x) \\sim GP(m(x), k(x,x'))$\n",
    "The mean and  the variance are functions; for any $x$, I can compute the mean and the variance. And I can take infinitely many points and draw a value from the GP, and it would be like drawing a function (though, in reality, im sampling a lot of points)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
