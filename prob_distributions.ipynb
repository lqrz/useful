{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli distribution\n",
    "\n",
    "A Bernoulli random variable indicates if the outcome is a success or a failure; that is, Bernoulli random variables can only take the value 1 or 0 (exclusively), with probabilities _p_ and _(1-p)_, respectively.\n",
    "\n",
    "It is important to note that the probability of success _remains constant across experiments_!\n",
    "\n",
    "Thus, the distribution is:\n",
    "\n",
    "$$\n",
    "P(X=x) = p^x \\times (1-p)^{1-x}\n",
    "$$\n",
    "\n",
    "#### Mean\n",
    "\n",
    "Its mean is, according to the definition:\n",
    "$$\n",
    "\\mu = \\sum x p(x) = 0 p(0) + 1 p(1) = p(1) = p^1 \\times (1-p)^0 = p\n",
    "$$\n",
    "\n",
    "#### Variance\n",
    "Following the definition:\n",
    "$$\n",
    "\\sigma^2 = \\mathbf{E}[(x-\\mu)^2] = \\sum (x-\\mu)^2 p(x) = (-p)^2 p(0) + (1-p)^2 p(1) = p^2(1-p) + (1-p)^2p = p(1-p) (p + (1-p)) = p(1-p)\n",
    "$$\n",
    "\n",
    "#### Fundamental bridge\n",
    "\n",
    "It is interesting to note that the expectation of a Bernoulli is just equal to the probability of success. This way, we can relate expectations to probabilities, know as the _Fundamental bridge_ (Ref: https://www.youtube.com/watch?v=LX2q356N2rU).\n",
    "\n",
    "This is useful cause we can narrow down the computation of expectations of much more complicated random variables to the expectation of Bernoullis, by means of linearity of expectations, fundamental bridge, and symmetry. __See the Hypergeometric distribution section below.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution\n",
    "\n",
    "Binomial random variables indicates the number of successes in a repeated independent experiment. We can think of the Binomial random variable (_X_) as summing over the Bernoulli random variables ($I_i$) (cause they are gonna act as indicator variables):\n",
    "$$\n",
    "X = I_1 + I_2 + I_3 + ... + I_n\n",
    "$$\n",
    "\n",
    "The Binomial random variable is like executing a bernoulli distribution several times (e.g. getting _x_ heads out of _n_ coin flips).\n",
    "\n",
    "Thus, it can be thought of the probability of one successfull sequence outcome $p^x (1-p)^{1-x}$, and then adjust by the reordering $\\binom{n}{x}$. Its distribution is:\n",
    "$$\n",
    "P(X=x) = \\binom{n}{x} p^x (1-p)^{1-x}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Binomial theorem\n",
    "This distribution owes its name to the Binomial theorem, which states:\n",
    "$$\n",
    "(p+q)^n = \\sum_n \\binom{p}{x} p^x q^{1-x}\n",
    "$$\n",
    "\n",
    "With this theorem we can prove that the Binomial distribution sums to 1 ($\\sum_n \\binom{p}{x} p^x q^{1-x} = (p+q)^n = (p + (1-p))^n = 1^n = 1$).\n",
    "\n",
    "#### Mean\n",
    "The mean of the binomial is easy to get if we rely on the Bernoulli mean:\n",
    "$$\n",
    "\\mathbf{E}[X] = \\mathbf{E}[I_1 + I_2 + I_3 + ... + I_n] = n \\times p\n",
    "$$\n",
    "\n",
    "The mean can also be computed using the definition of the expectation, and the Binomial theorem:\n",
    "$$\n",
    "\\mathbf{E}[X] = \\sum x p(x) = \\sum_0^n x \\binom{n}{x} p^x (1-p)^{n-x} = \\sum_1^n x \\binom{n}{x} p^x (1-p)^{n-x} = \\sum_1^n x \\frac{n!}{x! (n-x)!} p^x (1-p)^{n-x} = \\sum_1^n x \\frac{n!}{x (x-1)! (n-x)!} p^x (1-p)^{n-x} = \\sum_1^n np \\frac{(n-1)!}{(x-1)! (n-x)!} p^{x-1} (1-p)^{n-x} = np \\sum_1^n \\binom{n-1}{x-1} p^{x-1} (1-p)^{n-x} = \\text{(Making j=x-1)} = np \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{n-(j+1)} = \\text{(Using the Binomial theorem)} = np\n",
    "$$\n",
    "\n",
    "#### Variance\n",
    "Again, using the Bernoulli results:\n",
    "$$\n",
    "\\mathbf{VAR}[X] = \\mathbf{VAR}[I_1 + I_2 + I_3 + ... + I_n] = n \\times p \\times (1-p)\n",
    "$$\n",
    "\n",
    "Here is the derivation of these params, without relying on the Bernoulli variables (https://www.youtube.com/watch?v=8fqkQRjcR1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial distribution\n",
    "\n",
    "Building on the Binomial case, now lets suppose that our experiment can have more than 2 outcomes (0 or 1). We will now have _k_ events, and each one occur $n_i$ times ($\\sum_i n_i = n$), with probabilities $p_i$ ($\\sum p_i = 1$).\n",
    "\n",
    "Still the probabilities remain unchanged accross experiments!\n",
    "\n",
    "The distribution is:\n",
    "$$\n",
    "P(X_1=x_1, X_2=x_2, ..., X_k=x_k) = \\binom{n}{x_1,x_2,...,x_k} p_1^{x_1} \\times p_2^{x_2} \\times ... \\times p_k^{x_k}\n",
    "$$\n",
    "\n",
    "#### Mean\n",
    "\n",
    "\n",
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypergeometric distribution\n",
    "\n",
    "While the Binomial required independent experiments (sampling with replacement, so that the probability of success remains constant), the Hypergeometric does not.\n",
    "\n",
    "The way of obtaining the probability of an Hypergeometric variable is by counting the total number of successes vs the total number of combinations:\n",
    "\n",
    "#### Example: Computing probabilities using a Hypergeometric distribution\n",
    "There are 8 red balls, 3 yellow balls, and 9 white balls. What is the probability of sampling 6 balls and obtaining 2 red, 1 yellow, and 3 white.\n",
    "$$\n",
    "p = \\frac{\\text{Total ways of reordering the desired sequence}}{\\text{Total combinations}} = \\frac{ \\binom{8}{2} \\binom{3}{1} \\binom{9}{3} }{ \\binom{20}{6} } = \\binom{6}{2,1,3} \\times P(X_1=r) \\times P(X_2=r/X_1=r) \\times P(X_3=y) \\times P(X_4=w) \\times P(X_5=w/X_4=w) \\times P(X_6=w/P_4=w, P_5=w) = \\frac{6!}{2!1!3!} \\times \\frac{8}{20} \\times \\frac{7}{19} \\times \\frac{3}{18} \\times \\frac{9}{17} \\times \\frac{8}{16} \\times \\frac{7}{15} = 0.182\n",
    "$$\n",
    "\n",
    "#### Example: Expectation of an Hypergeometric random var using the Fundamental bridge\n",
    "\n",
    "X is the nr of aces drawn in a 5-card hand, and we need to find its expectation.\n",
    "\n",
    "If we think of each card as an indicator random variable (that is, a Bernoulli random var), then:\n",
    "$$\n",
    "\\mathbf{E}[X] = \\text{by ind. random vars} = \\mathbf{E}[I_1 + I_2 + I_3 + I_4 + I_5] = \\text{by linearity and symmetry} = 5 \\times \\mathbf{E}[I] = \\text{by fund. bridge} = 5 \\times P(Ace) = 5 \\times \\frac{4}{52}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "Poisson random variables measure the number of times an event occurs within a certain range (time interval, distance, area, volume, etc); e.g. \"number of rats per $m^2$\", \"number of cars passing every 10 minutes\", \"number of chips on a cookie\".\n",
    "\n",
    "The results of experiments could be _weakly dependent:_ that is, knowing that _A_ happened might not tell you much about _C_, but knowing that _A_ and _B_ happened might give you some info about _C_). And also, comparing to the Binomial, the probability of each event does not have to be fixed.\n",
    "\n",
    "One way of thinking of Poisson random variables (the number of events that occur), is that there is a large number of events which can lead to a success or failure, but which have a very low probability. For example, we can subdivide an area into very small pieces (making it a very large number), an each one will have a low probability of something happening in that exact piece (Ref: https://www.youtube.com/watch?v=TD1N4hxqMzY).\n",
    "From this last point, is where the Poisson can be related to the Binomial distribution (where $n \\rightarrow \\infty$ and $p \\rightarrow 0$).\n",
    "\n",
    "$$\n",
    "P(X=x; \\lambda t) = \\frac{e^{-\\lambda t}(\\lambda t)^x}{x!}\n",
    "$$\n",
    "\n",
    "#### Mean\n",
    "The mean of the Poisson is $\\lambda t$.\n",
    "\n",
    "#### Variance\n",
    "\n",
    "This is also $\\lambda t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
